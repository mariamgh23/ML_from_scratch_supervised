# ğŸš€ Machine Learning From Scratch

![Python](https://img.shields.io/badge/Python-3.11-blue)
![NumPy](https://img.shields.io/badge/NumPy-Vectorized-orange)
![Status](https://img.shields.io/badge/Status-Active-success)
![License](https://img.shields.io/badge/License-MIT-green)

A complete implementation of core Machine Learning algorithms **from scratch using only NumPy**, without relying on scikit-learn models.

This repository focuses on understanding how ML algorithms work internally by building them step-by-step from mathematical foundations.

---

## ğŸ¯ Project Goals

- Understand ML algorithms at mathematical level
- Implement models using only NumPy
- Build ensemble methods (Bagging & Boosting)
- Create custom evaluation metrics
- Compare models on real datasets (Iris & Digits)

---

# ğŸ§  Implemented Algorithms

## ğŸ“ˆ Regression
- Linear Regression (Gradient Descent based)

## ğŸ“Š Classification
- Logistic Regression (Binary & One-vs-Rest)
- Linear SVM (Binary & OVR)
- Decision Tree (Gini-based)
- Random Forest (Bootstrap Aggregation)
- XGBoost (Simplified Gradient Boosting)

## ğŸ“ Evaluation Metrics (From Scratch)
- Confusion Matrix
- Macro F1 Score

---

# ğŸ— Project Structure
ML-From-Scratch/
â”‚
â”œâ”€â”€ gradient_descent.py
â”œâ”€â”€ Linear_Regression.py
â”œâ”€â”€ Logestic_Regression.py
â”œâ”€â”€ SVM.py
â”œâ”€â”€ Simple_DecisionTree.py
â”œâ”€â”€ RandomForest.py
â”œâ”€â”€ XGBoost.py
â”œâ”€â”€ evaluation_metrics.py
â”‚
â”œâ”€â”€ iris_test_models.py
â”œâ”€â”€ digits_test_models.py
â”‚
â””â”€â”€ README.md

---

# ğŸ“Š Datasets Used

## ğŸŒ¸ Iris Dataset
- 150 samples
- 4 features
- 3 classes
- Multi-class classification

## âœï¸ Digits Dataset
- 1797 samples
- 64 features (8x8 image flattened)
- 10 classes (digits 0â€“9)
- Multi-class classification

---

# ğŸ”¬ Model Concepts

## Logistic Regression (OVR)
- Sigmoid activation
- Cross-entropy gradient descent
- One-vs-Rest for multi-class

## Linear SVM
- Hinge loss
- Maximum margin classifier
- OVR for multi-class

## Decision Tree
- Gini impurity
- Information Gain
- Recursive splitting

## Random Forest
- Bootstrap sampling
- Majority voting
- Ensemble learning (Bagging)

## XGBoost (Simplified)
- Gradient boosting concept
- Residual learning
- Sequential tree correction

---

# âš™ï¸ How It Works
Input Data
â†“
Train/Test Split
â†“
Model Training
â†“
Prediction
â†“
Confusion Matrix + F1 Score

---

# â–¶ï¸ How to Run

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/yourusername/ML-From-Scratch.git
cd ML-From-Scratch
```
### ğŸ“Š Example Output
```
===== Logistic Regression OVR =====
Confusion Matrix:
[[10  0  0]
 [ 0  9  1]
 [ 0  1  9]]
F1 Score: 0.93
```
### ğŸ§® Mathematical Foundations Covered

* Gradient Descent

* Hinge Loss

* Cross-Entropy Loss

* Gini Impurity

* Information Gain

* Bagging

* Boosting

* One-vs-Rest Strategy

### ğŸš€ Why This Repository Is Valuable

âœ” Demonstrates deep ML understanding

âœ” Shows ability to implement algorithms from first principles

âœ” Covers linear, tree-based, and ensemble methods

âœ” Includes evaluation metrics from scratch

âœ” Strong portfolio project for ML engineering roles

### ğŸ“Œ Future Improvements

* Full regression trees for true XGBoost

* Feature scaling module

* Early stopping

* Cross-validation implementation

### ğŸ¤ Contributions

Contributions are welcome!

You can:

Improve performance

Optimize vectorization

Add new ML algorithms

Enhance documentation


### ğŸ“œ License

This project is licensed under the MIT License.

### â­ If You Like This Project

Give it a star â­ and feel free to connect!
